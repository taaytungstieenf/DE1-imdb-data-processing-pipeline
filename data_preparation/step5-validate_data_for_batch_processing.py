import findspark
findspark.init("/opt/spark")

import os
from pyspark.sql import SparkSession
from contextlib import redirect_stdout

# Khá»Ÿi táº¡o Spark
spark = SparkSession.builder.appName("Validate Fact Movie Data").getOrCreate()

# XÃ¡c Ä‘á»‹nh Ä‘Æ°á»ng dáº«n
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
FACT_DATA_PATH = os.path.join(PROJECT_ROOT, "data_validation", "data_for_batch", "fact_movie_full.parquet")
LOG_PATH = os.path.join(PROJECT_ROOT, "data_validation", "validation_of_step5_fact_movie_full_dataset.log")

# Äáº£m báº£o thÆ° má»¥c data_validation tá»“n táº¡i
os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)

# Äá»c dá»¯ liá»‡u
df = spark.read.parquet(FACT_DATA_PATH)

# Redirect stdout Ä‘á»ƒ ghi log
with open(LOG_PATH, "w", encoding="utf-8") as f:
    with redirect_stdout(f):
        print("=== âœ… VALIDATION REPORT: fact_movie_full.parquet ===\n")

        # Schema
        print("ğŸ”¹ Schema:")
        df.printSchema()

        # Tá»•ng sá»‘ dÃ²ng
        print("\nğŸ”¹ Tá»•ng sá»‘ dÃ²ng:")
        print(df.count())

        # Nulls theo cá»™t
        print("\nğŸ”¹ Sá»‘ lÆ°á»£ng giÃ¡ trá»‹ NULL theo cá»™t:")
        for col_name in df.columns:
            count_null = df.filter(df[col_name].isNull()).count()
            print(f"   {col_name}: {count_null} nulls")

        # PhÃ¢n phá»‘i theo titleType
        print("\nğŸ”¹ PhÃ¢n phá»‘i theo titleType:")
        df.groupBy("titleType").count().orderBy("count", ascending=False).show(truncate=False)

        # PhÃ¢n phá»‘i theo startYear
        print("\nğŸ”¹ PhÃ¢n phá»‘i theo startYear:")
        df.groupBy("startYear").count().orderBy("startYear").show(20, truncate=False)

        # PhÃ¢n phá»‘i theo averageRating
        print("\nğŸ”¹ PhÃ¢n phá»‘i theo averageRating:")
        df.groupBy("averageRating").count().orderBy("averageRating").show(20, truncate=False)

        print(f"\nâœ… ÄÃ£ ghi bÃ¡o cÃ¡o vÃ o: {LOG_PATH}")

spark.stop()
