import findspark
findspark.init("/opt/spark")

import os
from pyspark.sql import SparkSession
from contextlib import redirect_stdout

# Táº¡o SparkSession
spark = SparkSession.builder.appName("Query Parquet Tables").getOrCreate()

# ÄÆ°á»ng dáº«n dá»± Ã¡n
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_PARQUET_DIR = os.path.join(PROJECT_ROOT, "data_storage", "data_parquet")
OUTPUT_LOG = os.path.join(PROJECT_ROOT, "data_validation", "validation_step3_parquet_data_information.log")

# Äáº£m báº£o thÆ° má»¥c data_validation tá»“n táº¡i
os.makedirs(os.path.dirname(OUTPUT_LOG), exist_ok=True)

# Láº¥y danh sÃ¡ch file parquet
parquet_files = [f for f in os.listdir(DATA_PARQUET_DIR) if f.endswith(".parquet")]

# Má»Ÿ file log vÃ  redirect stdout vÃ o Ä‘Ã³
with open(OUTPUT_LOG, "w", encoding="utf-8") as f:
    with redirect_stdout(f):
        if not parquet_files:
            print("âŒ KhÃ´ng tÃ¬m tháº¥y file .parquet nÃ o.")
        else:
            for file in parquet_files:
                table_name = file.replace(".parquet", "")
                file_path = os.path.join(DATA_PARQUET_DIR, file)

                print("\n==============================")
                print(f"ğŸ“¦ Báº£ng: {table_name}")
                print(f"ğŸ“ File: {file_path}")

                # Äá»c parquet
                df = spark.read.parquet(file_path)

                print("ğŸ”¹ Schema:")
                df.printSchema()

                print("ğŸ”¹ 10 dÃ²ng Ä‘áº§u:")
                df.show(10, truncate=False)

                total = df.count()
                print(f"ğŸ”¹ Sá»‘ dÃ²ng: {total}")

print(f"\nâœ… ÄÃ£ ghi káº¿t quáº£ vÃ o: {OUTPUT_LOG}")

# Dá»«ng Spark
spark.stop()
